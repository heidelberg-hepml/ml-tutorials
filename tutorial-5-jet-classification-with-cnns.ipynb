{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet classification with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial you should use a combination of your lecture notes and the pytorch tutorial at:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    " - Imports and plotting set-up\n",
    " - Preprocessing jet images\n",
    " - Visualising the data\n",
    " - Datasets and dataloaders\n",
    " - Building the CNN\n",
    " - Training the CNN\n",
    " - Plot train and validation losses\n",
    " - Study the results\n",
    " - CNNs and translation invariance\n",
    " - Why are CNNs better than fully connected networks here?\n",
    "     - how many trainable parameters does the CNN have?\n",
    "     - if you like, build a fully connected network with approximately the same number of parameters as your CNN, train it, and compare the performance to the CNN.\n",
    " - How could we improve the results we have here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use a public dataset, the download procedure is described below. Alternatively, you can directly download the reduced-size dataset from the ITP website with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to download dataset 2 ('top tagging', used in tutorials [5, 6, 7, 8, 9]) from https://www.thphys.uni-heidelberg.de/~plehn/pics/toptagging-short.zip to toptagging-short.zip\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:03<00:00,  1.86it/s]\n",
      "Successfully extracted files from toptagging-short.zip\n",
      "data\n",
      "README.md\n",
      "simple-graph-image.png\n",
      "tutorial-10-cwola.ipynb\n",
      "tutorial-11-gans.ipynb\n",
      "tutorial-12-flows.ipynb\n",
      "tutorial-13-uncertainties-with-flows.ipynb\n",
      "tutorial-2-amplitude-regression.ipynb\n",
      "tutorial-3-reg-and-prep.ipynb\n",
      "tutorial-4-bayesian-regression.ipynb\n",
      "tutorial-5-jet-classification-with-cnns.ipynb\n",
      "tutorial-6-bayesian-cnns.ipynb\n",
      "tutorial-7-dynamical-graph-convolutional-neural-nets.ipynb\n",
      "tutorial-8-transformers.ipynb\n",
      "tutorial-9-autoencoders.ipynb\n"
     ]
    }
   ],
   "source": [
    "!python3 data/get_data.py 2 data\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data we'll use the erum-data framework outline is this paper:\n",
    "\n",
    "https://arxiv.org/pdf/2107.00656.pdf\n",
    "\n",
    "The erum data python package can be installed using the command below:\n",
    "\n",
    "`python3 -m pip install git+https://github.com/erum-data-idt/pd4ml`\n",
    "\n",
    "A notebook showing how to load the top-tagging data using this package can be found here:\n",
    "\n",
    "https://github.com/erum-data-idt/pd4ml/blob/main/examples/1_top_plots.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pd4ml import TopTagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erum data includes a desription of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top tagging dataset.\n",
      "\n",
      "    Description:\n",
      "    14 TeV, hadronic tops for signal, QCD djets background, Delphes ATLAS detector card with Pythia. No MPI/pile-up included\n",
      "    Particle-flow entries (produced by Delphes E-flow) have been clustered into anti-kT 0.8 jets in the pT range [550,650].\n",
      "    All top jets are matched to a parton-level top within ∆R = 0.8, and to all top decay partons within 0.8. Also,|eta|_jet < 2 has been required.\n",
      "\n",
      "    Ref:\n",
      "    Deep-learned Top Tagging with a Lorentz Layer by A Butter, G Kasieczka, T and M Russell (arXiv: 1707.08966)\n",
      "\n",
      "    Dataset shape:\n",
      "    ~2M events have been stored divided between training (~1.6M) and test (~400k)) and the shape of the dataset is (# of events, 200, 4).\n",
      "    The feature represent the leading 200 jet constituent four-momenta, with zero-padding for jets that have less than 200.\n",
      "    Constituents are sorted by pT, with the highest pT one first.\n",
      "\n",
      "    The second dataset that is included is just a flag \"ttv\" to identify what the event was before the reshaping operated by us. Here a legenda:\n",
      "        0 = training event;\n",
      "        1 = test event;\n",
      "        2 = validation event;\n",
      "\n",
      "    Note that in the current splitting of the dataset, training and validation events have been merged together as a unique training dataset. So for most intents and purposes one should just train the model on the first dataset and omit the second 'ttv' dataset altogether.\n",
      "\n",
      "    The set label are 0 for QCD and 1 for top.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "TopTagging.print_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below loads the dataset, downloading it if it is not already there.\n",
    "\n",
    "Warning: these files are large ($\\simeq1.5$gb) and downloading them takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, y_train_f = TopTagging.load('train', path = 'tutorial-3-data')\n",
    "X_test_f, y_test_f = TopTagging.load('test', path = 'tutorial-3-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = X_train_f[0]\n",
    "X_test_f = X_test_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:\n",
      "X_train - (1614000, 200, 4)\n",
      "y_train - (1614000,)\n",
      "X_test  - (404000, 200, 4)\n",
      "y_test  - (404000,)\n"
     ]
    }
   ],
   "source": [
    "print( \"shapes:\" )\n",
    "print( \"X_train - \" + str( np.shape( X_train_f ) ) )\n",
    "print( \"y_train - \" + str( np.shape( y_train_f ) ) )\n",
    "print( \"X_test  - \" + str( np.shape( X_test_f ) ) )\n",
    "print( \"y_test  - \" + str( np.shape( y_test_f ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too much for us, let's cut it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nj = 30000\n",
    "X_train = X_train_f[ 0:nj ]\n",
    "y_train = y_train_f[ 0:nj ]\n",
    "X_test = X_test_f[ 0:nj ]\n",
    "y_test = y_test_f[ 0:nj ]\n",
    "X_val = X_train_f[ -nj: ]\n",
    "y_val = y_train_f[ -nj: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:\n",
      "X_train - (30000, 200, 4)\n",
      "y_train - (30000,)\n",
      "X_test  - (30000, 200, 4)\n",
      "y_test  - (30000,)\n",
      "X_val  - (30000, 200, 4)\n",
      "y_val  - (30000,)\n"
     ]
    }
   ],
   "source": [
    "print( \"shapes:\" )\n",
    "print( \"X_train - \" + str( np.shape( X_train ) ) )\n",
    "print( \"y_train - \" + str( np.shape( y_train ) ) )\n",
    "print( \"X_test  - \" + str( np.shape( X_test ) ) )\n",
    "print( \"y_test  - \" + str( np.shape( y_test ) ) )\n",
    "print( \"X_val  - \" + str( np.shape( X_val ) ) )\n",
    "print( \"y_val  - \" + str( np.shape( y_val ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this out so we have the shorter versions in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( \"data/toptagging-short/x_train_short.npy\", X_train )\n",
    "np.save( \"data/toptagging-short/y_train_short.npy\", y_train )\n",
    "np.save( \"data/toptagging-short/x_test_short.npy\", X_test )\n",
    "np.save( \"data/toptagging-short/y_test_short.npy\", y_test )\n",
    "np.save( \"data/toptagging-short/x_val_short.npy\", X_val )\n",
    "np.save( \"data/toptagging-short/y_val_short.npy\", y_val )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys\n",
    "\n",
    "labelfont = FontProperties()\n",
    "labelfont.set_family('serif')\n",
    "labelfont.set_name('Times New Roman')\n",
    "labelfont.set_size(14)\n",
    "\n",
    "axislabelfont = FontProperties()\n",
    "axislabelfont.set_family('serif')\n",
    "axislabelfont.set_name('Times New Roman')\n",
    "axislabelfont.set_size(22)\n",
    "\n",
    "tickfont = FontProperties()\n",
    "tickfont.set_family('serif')\n",
    "tickfont.set_name('Times New Roman')\n",
    "tickfont.set_size(16)\n",
    "\n",
    "axisfontsize = 16\n",
    "labelfontsize = 16\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the (already collected) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( \"data/toptagging-short/x_train_short.npy\")\n",
    "y_train = np.load( \"data/toptagging-short/y_train_short.npy\")\n",
    "X_test = np.load( \"data/toptagging-short/x_test_short.npy\")\n",
    "y_test = np.load( \"data/toptagging-short/y_test_short.npy\")\n",
    "X_val = np.load( \"data/toptagging-short/x_val_short.npy\")\n",
    "y_val = np.load( \"data/toptagging-short/y_val_short.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing jet images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the jets to $(p_T,\\eta,\\phi)$ format, preprocessing them with rotations and translations, and convert them to an image format.\n",
    "\n",
    "We will preprocess the data such that:\n",
    "- the jets are centred at $(\\eta,\\phi)=(0.0,0.0)$\n",
    "- the principle axis of each jet points in the same direction\n",
    "- the quadrant of the jets with the most $p_T$ is the same for each jet\n",
    "- the jet $p_T$'s are normalised.\n",
    "\n",
    "We need some functions for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial settings\n",
    "__n_warning__ = 0.7\n",
    "n_shift_phi, n_shift_eta = 0, 0\n",
    "\n",
    "# Grid settings\n",
    "xpixels = np.arange(-2.6, 2.6, 0.029)\n",
    "ypixels = np.arange(-np.pi, np.pi, 0.035)\n",
    "\n",
    "# Calculate the pseudorapidity of pixel entries\n",
    "def eta (pT, pz):\n",
    "    small = 1e-10\n",
    "    small_pT = (np.abs(pT) < small)\n",
    "    small_pz = (np.abs(pz) < small)\n",
    "    not_small = ~(small_pT | small_pz)\n",
    "    theta = np.arctan(pT[not_small]/pz[not_small])\n",
    "    theta[theta < 0] += np.pi\n",
    "    etas = np.zeros_like(pT)\n",
    "    etas[small_pz] = 0\n",
    "    etas[small_pT] = 1e-10\n",
    "    etas[not_small] = np.log(np.tan(theta/2))\n",
    "    return etas\n",
    "\n",
    "# Calculate the azimuthal angle of pixel entries\n",
    "def phi (px, py):\n",
    "    \"\"\"\n",
    "    phis are returned in rad., np.arctan(0,0)=0 -> zero constituents set to -np.pi\n",
    "    \"\"\"\n",
    "    phis = np.arctan2(py,px)\n",
    "    phis[phis < 0] += 2*np.pi\n",
    "    phis[phis > 2*np.pi] -= 2*np.pi\n",
    "    phis = phis - np.pi \n",
    "    return phis\n",
    "\n",
    "# function to calculate masses\n",
    "def mass (E,px,py,pz):\n",
    "    mass = np.sqrt(np.maximum(0.,E**2-px**2-py**2-pz**2))\n",
    "    return mass\n",
    "\n",
    "# function to return the image momenta for centroid and principal axis\n",
    "def img_mom (x, y, weights, x_power, y_power):\n",
    "    return ((x**x_power)*(y**y_power)*weights).sum()\n",
    "\n",
    "# returns the jet image\n",
    "def orig_image (etas, phis, es):\n",
    "    \"\"\"\n",
    "    Gives the value on grid with minimal distance,\n",
    "    eg. for xpixel = (0,1,2,3,..) eta=1.3 -> xpixel=1, eta=1.6 ->xpixel=2\n",
    "    \"\"\"\n",
    "    z = np.zeros((etas.shape[0],len(xpixels),len(ypixels)))\n",
    "    in_grid = ~((etas < xpixels[0]) | (etas > xpixels[-1]) | (phis < ypixels[0]) | (phis > ypixels[-1]))\n",
    "    xcoords = np.argmin(np.abs(etas[:,None,:] - xpixels[None,:,None]),axis=1)\n",
    "    ycoords = np.argmin(np.abs(phis[:,None,:] - ypixels[None,:,None]),axis=1)\n",
    "    ncoords = np.repeat(np.arange(etas.shape[0])[:,None],etas.shape[1],axis=1)\n",
    "    z[ncoords[in_grid],ycoords[in_grid],xcoords[in_grid]] = es[in_grid]\n",
    "    return z\n",
    "\n",
    "# preprocess the jet\n",
    "def preprocessing( x ,y, weights, rotate=True, flip=True ):\n",
    "    \"\"\"\n",
    "    (x,y) are the coordinates and weights the corresponding values, shifts\n",
    "    centroid to origin, rotates image, so that principal axis is vertical,\n",
    "    flips image, so that most weights lay in (x<0, y>0)-plane.\n",
    "    Method for calculating principal axis (similar to tensor of inertia):\n",
    "    https://en.wikipedia.org/wiki/Image_moment\n",
    "    here: y=phi, phi has modulo 2*np.pi but it's not been taken care of hear,\n",
    "    so possible issues with calculating the centroid\n",
    "    -> pre-shifting of events outside of this function solves the problem\n",
    "    for iamge-data with Delta_phi < 2*np.pi\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift\n",
    "    x_centroid = img_mom(x, y, weights, 1, 0) / weights.sum()\n",
    "    y_centroid = img_mom(x, y, weights, 0, 1)/ weights.sum()\n",
    "    x = x - x_centroid\n",
    "    y = y - y_centroid\n",
    "\n",
    "    # Check if shifting worked, there can be problems with modulo variables like phi (y)\n",
    "    # x and y are sorted after highest weight, 0-comp. gives hottest event\n",
    "    # for Jet-like Images Centroid should be close to hottest constituen (pT-sorted arrays)  \n",
    "    global n_shift_phi\n",
    "    global n_shift_eta\n",
    "    if np.abs(x[0]) > __n_warning__:\n",
    "        n_shift_eta += 1  \n",
    "    if np.abs(y[0]) > __n_warning__:\n",
    "        n_shift_phi += 1       \n",
    "\n",
    "    if rotate:\n",
    "        #Ccovariant matrix, eigenvectors corr. to principal axis\n",
    "        u11 = img_mom(x, y, weights, 1, 1) / weights.sum()\n",
    "        u20 = img_mom(x, y, weights, 2, 0) / weights.sum()\n",
    "        u02 = img_mom(x, y, weights, 0, 2) / weights.sum()\n",
    "        cov = np.array([[u20, u11], [u11, u02]])\n",
    "\n",
    "        # Eigenvalues and eigenvectors of covariant matrix\n",
    "        evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "        # Sorts the eigenvalues, v1, [::-1] turns array around, \n",
    "        sort_indices = np.argsort(evals)[::-1]\n",
    "        e_1 = evecs[:, sort_indices[0]]  # Eigenvector with largest eigenvalue\n",
    "        e_2 = evecs[:, sort_indices[1]]\n",
    "\n",
    "        # Theta to x_asix, arctan2 gives correct angle\n",
    "        theta = np.arctan2(e_1[0], e_1[1])\n",
    "  \n",
    "        # Rotation, so that princple axis is vertical\n",
    "        # anti-clockwise rotation matrix\n",
    "        rotation = np.matrix([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "        transformed_mat = rotation * np.stack([x,y])\n",
    "        x_rot, y_rot = transformed_mat.A\n",
    "    else: \n",
    "        x_rot, y_rot = x, y\n",
    "  \n",
    "    # Flipping\n",
    "    n_flips = 0\n",
    "    if flip:\n",
    "        if weights[x_rot<0.].sum() < weights[x_rot>0.].sum():\n",
    "            x_rot = -x_rot\n",
    "            n_flips  += 1\n",
    "        if weights[y_rot<0.].sum() > weights[y_rot>0.].sum():\n",
    "            y_rot = -y_rot\n",
    "            n_flips += 1\n",
    "            \n",
    "    return x_rot, y_rot\n",
    "\n",
    "\n",
    "# function to convert the jet to an image\n",
    "def constit_to_img( jets, n_constit, norm, rotate, flip ):\n",
    "    \n",
    "    print( \"Crop constituents\" )\n",
    "    jets = jets[:,0:n_constit,:]\n",
    "    \n",
    "    print( \"Calculating pT\" )\n",
    "    E     = jets[:,:,0]\n",
    "    pxs   = jets[:,:,1]\n",
    "    pys   = jets[:,:,2]\n",
    "    pzs   = jets[:,:,3]\n",
    "    pT    = np.sqrt(pxs**2+pys**2)\n",
    "    \n",
    "    print( \"Calculating eta\" )\n",
    "    etas  = eta(pT,pzs)\n",
    "    \n",
    "    print( \"Calculating phi\" )\n",
    "    phis  = phi(pxs,pys)\n",
    "    \n",
    "    print( \"Calculating the mass\" )\n",
    "    E_tot = E.sum(axis=1)\n",
    "    px_tot = pxs.sum(axis=1)\n",
    "    py_tot = pys.sum(axis=1)\n",
    "    pz_tot = pzs.sum(axis=1)\n",
    "    j_mass = mass(E_tot, px_tot, py_tot, pz_tot)\n",
    "    \n",
    "    print( \"Pre-shifting the phis\" )\n",
    "    phis = (phis.T - phis[:,0]).T\n",
    "    phis[phis < -np.pi] += 2*np.pi\n",
    "    phis[phis > np.pi] -= 2*np.pi\n",
    "    \n",
    "    print( \"Using pT as weight\" )\n",
    "    weights = pT\n",
    "    \n",
    "    print( \"Preprocessing\" )\n",
    "    \n",
    "    for i in range( np.shape(etas)[0] ):\n",
    "        etas[i,:], phis[i,:] = preprocessing( etas[i,:], phis[i,:], weights[i,:], rotate, flip )\n",
    "    \n",
    "    print( \"Creating images\" )\n",
    "    z_ori = orig_image(etas, phis, weights)\n",
    "    \n",
    "    #return z_ori\n",
    "        \n",
    "    print( \"Cropping and normalising\" )\n",
    "    n_crop = 40\n",
    "    z_new = np.zeros( (z_ori.shape[0],n_crop, n_crop) )\n",
    "    for i in range(z_ori.shape[0]):\n",
    "        Npix = z_ori[i,:,:].shape\n",
    "        z_new[i,:,:] = z_ori[i, int(Npix[0]/2-n_crop/2) : int(Npix[0]/2+n_crop/2), int(Npix[1]/2-n_crop/2) : int(Npix[1]/2+n_crop/2) ]\n",
    "        if norm:\n",
    "            z_sum = z_new[i,:,:].sum()\n",
    "            if z_sum != 0.:\n",
    "                z_new[i,:,:] = z_new[i,:,:]/z_sum\n",
    "    \n",
    "    print( \"Reshaping\" )\n",
    "    z_out = z_new.reshape( (z_new.shape[0],-1) )\n",
    "    \n",
    "    return z_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop constituents\n",
      "Calculating pT\n",
      "Calculating eta\n",
      "Calculating phi\n",
      "Calculating the mass\n",
      "Pre-shifting the phis\n",
      "Using pT as weight\n",
      "Preprocessing\n",
      "Creating images\n",
      "Cropping and normalising\n",
      "Reshaping\n"
     ]
    }
   ],
   "source": [
    "z_train = constit_to_img( X_train, 50, True, True, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( z_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29999.999999999964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop constituents\n",
      "Calculating pT\n",
      "Calculating eta\n",
      "Calculating phi\n",
      "Calculating the mass\n",
      "Pre-shifting the phis\n",
      "Using pT as weight\n",
      "Preprocessing\n",
      "Creating images\n",
      "Cropping and normalising\n",
      "Reshaping\n"
     ]
    }
   ],
   "source": [
    "z_test = constit_to_img( X_test, 50, True, True, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1600)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( z_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000.000000000065"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop constituents\n",
      "Calculating pT\n",
      "Calculating eta\n",
      "Calculating phi\n",
      "Calculating the mass\n",
      "Pre-shifting the phis\n",
      "Using pT as weight\n",
      "Preprocessing\n",
      "Creating images\n",
      "Cropping and normalising\n",
      "Reshaping\n"
     ]
    }
   ],
   "source": [
    "z_val = constit_to_img( X_val, 50, True, True, True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these jet images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = z_train[ np.where(y_train==1) ]\n",
    "bkg = z_train[ np.where(y_train==0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1453ecd30>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( sig[0].reshape( (40,40) ), cmap=\"gist_heat_r\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 1, 2, figsize=(9,5) )\n",
    "\n",
    "axs[0].imshow( sig[0].reshape( (40,40) ), cmap=\"gist_heat_r\" )\n",
    "axs[0].set_xlabel( \"$\\eta$\", fontproperties=axislabelfont )\n",
    "axs[0].set_ylabel( \"$\\phi$\", fontproperties=axislabelfont )\n",
    "ticks0 = [ int(x) for x in axs[0].get_yticks() ]\n",
    "axs[0].set_yticklabels( ticks0, fontproperties=tickfont )\n",
    "axs[0].set_xticklabels( ticks0, fontproperties=tickfont )\n",
    "\n",
    "axs[1].imshow( bkg[0].reshape( (40,40) ), cmap=\"gist_heat_r\" )\n",
    "axs[1].set_xlabel( \"$\\eta$\", fontproperties=axislabelfont )\n",
    "axs[1].set_ylabel( \"$\\phi$\", fontproperties=axislabelfont )\n",
    "ticks1 = [ int(x) for x in axs[1].get_yticks() ]\n",
    "axs[1].set_yticklabels( ticks1, fontproperties=tickfont )\n",
    "axs[1].set_xticklabels( ticks1, fontproperties=tickfont )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 1, 2, figsize=(9,5) )\n",
    "\n",
    "axs[0].imshow( sig.mean(0).reshape( (40,40) ), cmap=\"gist_heat_r\" )\n",
    "axs[0].set_xlabel( \"$\\eta$\", fontproperties=axislabelfont )\n",
    "axs[0].set_ylabel( \"$\\phi$\", fontproperties=axislabelfont )\n",
    "ticks0 = [ int(x) for x in axs[0].get_yticks() ]\n",
    "axs[0].set_yticklabels( ticks0, fontproperties=tickfont )\n",
    "axs[0].set_xticklabels( ticks0, fontproperties=tickfont )\n",
    "\n",
    "axs[1].imshow( bkg.mean(0).reshape( (40,40) ), cmap=\"gist_heat_r\" )\n",
    "axs[1].set_xlabel( \"$\\eta$\", fontproperties=axislabelfont )\n",
    "axs[1].set_ylabel( \"$\\phi$\", fontproperties=axislabelfont )\n",
    "ticks1 = [ int(x) for x in axs[1].get_yticks() ]\n",
    "axs[1].set_yticklabels( ticks1, fontproperties=tickfont )\n",
    "axs[1].set_xticklabels( ticks1, fontproperties=tickfont )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a minimal dataset similar to the last we have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_data( Dataset ):\n",
    "    \n",
    "    def __init__( self, imgs, labels ):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define images and labels as pytorch Tensors, we'll need to call them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = torch.Tensor(z_train.reshape(-1, 1, 40,40).astype('float32'))\n",
    "z_test = torch.Tensor(z_test.reshape(-1, 1, 40,40).astype('float32'))\n",
    "z_val = torch.Tensor(z_val.reshape(-1, 1, 40,40).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.Tensor(y_train).unsqueeze(-1)\n",
    "y_test = torch.Tensor(y_test).unsqueeze(-1)\n",
    "y_val = torch.Tensor(y_val).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the datasets and the dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_dataset = cnn_data(z_train, y_train.float())\n",
    "test_cnn_dataset = cnn_data(z_test, y_test.float())\n",
    "val_cnn_dataset = cnn_data(z_val, y_val.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_dataloader = DataLoader( train_cnn_dataset, batch_size=64, shuffle=True )\n",
    "test_cnn_dataloader = DataLoader( test_cnn_dataset, batch_size=64, shuffle=True )\n",
    "val_cnn_dataloader = DataLoader( val_cnn_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple CNN. We'll use two convolutions with a small number of channels. Two max-pooling layers are introduced to reduce the dimensionality of the images and the parameters of the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch=1, ch=4, out_dim=1, img_sz=40):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, ch, kernel_size=5, bias=True, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(ch, 1, kernel_size=5, bias=True, stride=1, padding=2)\n",
    "        self.max = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(int(img_sz*img_sz/16), out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.max,\n",
    "            nn.ReLU(),\n",
    "            self.conv2,\n",
    "            self.max,\n",
    "            self.flatten,\n",
    "            self.out,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available device, use GPU if possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our problem is two-class classification problem, we can use the Binary Cross-Entropy as a loss function:\n",
    "\n",
    "- BCE = $y_i \\log p(y_i) + (1-y_i) \\log (1-p(y_i)) $\n",
    "\n",
    "where $y_i$ is the true label, and $p(y_i)$ is the predicted probability of the event being a top jet.\n",
    "\n",
    "In the loss function we add the log-probability for the correct class. Therefore, increasing the fidelity of a pediction lowers the loss function as desired for an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch( dataloader, model, loss_fn, optimizer ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # pass data through network\n",
    "        pred = model(X)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn( pred, y )\n",
    "\n",
    "        # reset gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print the training loss every 100 updates\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print( f\"current batch loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pass( dataloader, model, loss_fn ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    num_batches = len( dataloader )\n",
    "    vl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model( X )\n",
    "            vl += loss_fn( pred, y ).item()\n",
    "\n",
    "    vl /= num_batches\n",
    "    print( f\"avg val loss per batch: {vl:>8f}\" )\n",
    "    \n",
    "    return vl\n",
    "\n",
    "def trn_pass( dataloader, model, loss_fn ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    num_batches = len( dataloader )\n",
    "    tl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model( X )\n",
    "            tl += loss_fn( pred, y ).item()\n",
    "\n",
    "    tl /= num_batches\n",
    "    print( f\"avg trn loss per batch: {tl:>8f}\" )\n",
    "    \n",
    "    return tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "ConvNet2D(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (max): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.695243  [    0/30000]\n",
      "current batch loss: 0.672724  [ 6400/30000]\n",
      "current batch loss: 0.489160  [12800/30000]\n",
      "current batch loss: 0.331076  [19200/30000]\n",
      "current batch loss: 0.405648  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.376601\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.327058  [    0/30000]\n",
      "current batch loss: 0.345435  [ 6400/30000]\n",
      "current batch loss: 0.360855  [12800/30000]\n",
      "current batch loss: 0.347693  [19200/30000]\n",
      "current batch loss: 0.228377  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366308\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.362703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.314880  [    0/30000]\n",
      "current batch loss: 0.380774  [ 6400/30000]\n",
      "current batch loss: 0.368533  [12800/30000]\n",
      "current batch loss: 0.383757  [19200/30000]\n",
      "current batch loss: 0.368283  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.361056\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.357297\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.446082  [    0/30000]\n",
      "current batch loss: 0.338378  [ 6400/30000]\n",
      "current batch loss: 0.436985  [12800/30000]\n",
      "current batch loss: 0.321613  [19200/30000]\n",
      "current batch loss: 0.339689  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.357864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.397848  [    0/30000]\n",
      "current batch loss: 0.483225  [ 6400/30000]\n",
      "current batch loss: 0.378444  [12800/30000]\n",
      "current batch loss: 0.331674  [19200/30000]\n",
      "current batch loss: 0.331144  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.355059\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.525848  [    0/30000]\n",
      "current batch loss: 0.421601  [ 6400/30000]\n",
      "current batch loss: 0.311799  [12800/30000]\n",
      "current batch loss: 0.393470  [19200/30000]\n",
      "current batch loss: 0.319346  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.351374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347771\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.386772  [    0/30000]\n",
      "current batch loss: 0.421336  [ 6400/30000]\n",
      "current batch loss: 0.310090  [12800/30000]\n",
      "current batch loss: 0.396873  [19200/30000]\n",
      "current batch loss: 0.238479  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346235\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.370216  [    0/30000]\n",
      "current batch loss: 0.231758  [ 6400/30000]\n",
      "current batch loss: 0.401726  [12800/30000]\n",
      "current batch loss: 0.320951  [19200/30000]\n",
      "current batch loss: 0.356692  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346773\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345027\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.177992  [    0/30000]\n",
      "current batch loss: 0.356903  [ 6400/30000]\n",
      "current batch loss: 0.256013  [12800/30000]\n",
      "current batch loss: 0.260376  [19200/30000]\n",
      "current batch loss: 0.348430  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.344736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341272\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.272287  [    0/30000]\n",
      "current batch loss: 0.296259  [ 6400/30000]\n",
      "current batch loss: 0.305685  [12800/30000]\n",
      "current batch loss: 0.431602  [19200/30000]\n",
      "current batch loss: 0.506795  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341702\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339481\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.231972  [    0/30000]\n",
      "current batch loss: 0.427093  [ 6400/30000]\n",
      "current batch loss: 0.243034  [12800/30000]\n",
      "current batch loss: 0.481643  [19200/30000]\n",
      "current batch loss: 0.287839  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342161\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.373169  [    0/30000]\n",
      "current batch loss: 0.226086  [ 6400/30000]\n",
      "current batch loss: 0.267045  [12800/30000]\n",
      "current batch loss: 0.360704  [19200/30000]\n",
      "current batch loss: 0.325686  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336057\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.432112  [    0/30000]\n",
      "current batch loss: 0.357131  [ 6400/30000]\n",
      "current batch loss: 0.218137  [12800/30000]\n",
      "current batch loss: 0.249661  [19200/30000]\n",
      "current batch loss: 0.452068  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334636\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.234071  [    0/30000]\n",
      "current batch loss: 0.284272  [ 6400/30000]\n",
      "current batch loss: 0.332047  [12800/30000]\n",
      "current batch loss: 0.465740  [19200/30000]\n",
      "current batch loss: 0.313288  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336135\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.230572  [    0/30000]\n",
      "current batch loss: 0.319564  [ 6400/30000]\n",
      "current batch loss: 0.427866  [12800/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.283122  [19200/30000]\n",
      "current batch loss: 0.310117  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.332139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.333100  [    0/30000]\n",
      "current batch loss: 0.482911  [ 6400/30000]\n",
      "current batch loss: 0.423123  [12800/30000]\n",
      "current batch loss: 0.271572  [19200/30000]\n",
      "current batch loss: 0.212228  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.402889  [    0/30000]\n",
      "current batch loss: 0.252676  [ 6400/30000]\n",
      "current batch loss: 0.327822  [12800/30000]\n",
      "current batch loss: 0.244846  [19200/30000]\n",
      "current batch loss: 0.376313  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.331981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.366992  [    0/30000]\n",
      "current batch loss: 0.330373  [ 6400/30000]\n",
      "current batch loss: 0.369281  [12800/30000]\n",
      "current batch loss: 0.212881  [19200/30000]\n",
      "current batch loss: 0.363193  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330657\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.329185\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197535  [    0/30000]\n",
      "current batch loss: 0.357906  [ 6400/30000]\n",
      "current batch loss: 0.308414  [12800/30000]\n",
      "current batch loss: 0.237891  [19200/30000]\n",
      "current batch loss: 0.415197  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328642\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.328009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.378469  [    0/30000]\n",
      "current batch loss: 0.374331  [ 6400/30000]\n",
      "current batch loss: 0.269741  [12800/30000]\n",
      "current batch loss: 0.352051  [19200/30000]\n",
      "current batch loss: 0.453563  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327727\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327236\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.403125  [    0/30000]\n",
      "current batch loss: 0.273426  [ 6400/30000]\n",
      "current batch loss: 0.211757  [12800/30000]\n",
      "current batch loss: 0.267450  [19200/30000]\n",
      "current batch loss: 0.465515  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.327473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.255399  [    0/30000]\n",
      "current batch loss: 0.382178  [ 6400/30000]\n",
      "current batch loss: 0.397896  [12800/30000]\n",
      "current batch loss: 0.238731  [19200/30000]\n",
      "current batch loss: 0.406675  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327069\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.325694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311239  [    0/30000]\n",
      "current batch loss: 0.279217  [ 6400/30000]\n",
      "current batch loss: 0.226866  [12800/30000]\n",
      "current batch loss: 0.302596  [19200/30000]\n",
      "current batch loss: 0.165658  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.323911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.321670  [    0/30000]\n",
      "current batch loss: 0.254680  [ 6400/30000]\n",
      "current batch loss: 0.354600  [12800/30000]\n",
      "current batch loss: 0.240654  [19200/30000]\n",
      "current batch loss: 0.336653  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.322875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.323861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.297305  [    0/30000]\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def separator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model = ConvNet2D(ch=4).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( train_cnn_dataloader, model, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( train_cnn_dataloader, model, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_cnn_dataloader, model, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot train and validation losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the results let's start by looking at the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "axs.plot( trn_losses, label=\"train loss\", color=c1 )\n",
    "axs.plot( val_losses, label=\"val   loss\", color=c2 )\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel( \"epoch\", fontproperties=axislabelfont )\n",
    "axs.set_ylabel( \"Binary CE\", fontproperties=axislabelfont )\n",
    "\n",
    "xticks = [ int(x) for x in axs.get_xticks() ]\n",
    "axs.set_xticklabels( xticks, fontproperties=tickfont )\n",
    "\n",
    "yticks = axs.get_yticks()\n",
    "axs.set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs.legend( loc='best', prop=tickfont )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg trn loss per batch: 0.300907\n",
      "avg val loss per batch: 0.308134\n",
      "avg val loss per batch: 0.302643\n",
      "-----------------------------------------------\n",
      "Final evaluation of performance:\n",
      "MSE-loss on train dataset: 0.3009068388928737\n",
      "MSE-loss on validation dataset: 0.30813442901380533\n",
      "MSE-loss on test dataset: 0.3026433375789158\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trn_loss = trn_pass( train_cnn_dataloader, model, loss_fn )\n",
    "val_loss = val_pass( test_cnn_dataloader, model, loss_fn )\n",
    "test_loss = val_pass( val_cnn_dataloader, model, loss_fn )\n",
    "\n",
    "separator()\n",
    "print(\"Final evaluation of performance:\")\n",
    "print(\"MSE-loss on train dataset: {}\".format(trn_loss))\n",
    "print(\"MSE-loss on validation dataset: {}\".format(val_loss))\n",
    "print(\"MSE-loss on test dataset: {}\".format(test_loss))\n",
    "separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these numbers don't show clearly what's the performance of our model.\n",
    "\n",
    "Let's try with the accuracy: $\\frac{\\mathbf{correct}}{\\mathbf{total}}\\cdot100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_size = 10000\n",
    "\n",
    "train_pred = model(z_train[:samp_size]).detach()\n",
    "test_pred = model(z_test[:samp_size]).detach()\n",
    "val_pred = model(z_val[:samp_size]).detach()\n",
    "\n",
    "train_pred_lab = torch.round(train_pred[:samp_size])\n",
    "test_pred_lab = torch.round(test_pred[:samp_size])\n",
    "val_pred_lab = torch.round(val_pred[:samp_size])\n",
    "\n",
    "train_correct = (train_pred_lab==y_train[:samp_size]).sum().item()\n",
    "test_correct = (test_pred_lab==y_test[:samp_size]).sum().item()\n",
    "val_correct = (val_pred_lab==y_val[:samp_size]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Evaluation of accuracy: \n",
      "Accuracy on training dataset: 86.0 %\n",
      "Accuracy on validation dataset: 86.9 %\n",
      "Accuracy on test dataset: 86.3 %\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "separator()\n",
    "print('Evaluation of accuracy: ')\n",
    "print('Accuracy on training dataset: {:.1f} %'.format(train_correct*100/len(train_pred)))\n",
    "print('Accuracy on validation dataset: {:.1f} %'.format(val_correct*100/len(val_pred)))\n",
    "print('Accuracy on test dataset: {:.1f} %'.format(test_correct*100/len(test_pred)))\n",
    "separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good accuracy for a really simple model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the results - plot the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible way to study our results is to plot a ROC curve. This is created by plotting the True Positive Rate against the False Positive Rate at different thresholds.TPR and FPR are defined by:\n",
    "\n",
    "- TPR = $\\frac{TP}{TP+FN}$;\n",
    "- FPR = $\\frac{FP}{FP+TN}$,\n",
    "\n",
    "namely the probability of correctly identyfing a signal against the probability of falsely rejecting a background event.\n",
    "\n",
    "When dealing with a background/signal test, it's usual to call the TPR signal efficiency and the FPR background rejection.\n",
    "\n",
    "Once we plot the ROC curve, we would like to quantify the performance according to some criteria.\n",
    "Two indices are mainly used:\n",
    "- AUC score, which is the Area Under the Curve;\n",
    "- Inverse mistag at low efficiency (e.g. 0.3)\n",
    "\n",
    "The AUC represents the probability of a random signal sample to have higher score than a background event.\n",
    "The inverse mistag hasn't a probabilistic interpretation, but higher values denote a better sensitivity to signal events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def closest_point(array, tpr_p=0.3):\n",
    "    dist = ((array-tpr_p)**2)\n",
    "    return np.argmin(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2143ef090bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msamp_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msamp_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrnd_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "fpr, tpr, th = roc_curve(test_true_lab, test_pred[:,1])\n",
    "auc_score = roc_auc_score(test_true_lab, test_pred[:,1])\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax[0].plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score) )\n",
    "ax[0].plot(rnd_class, rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].plot(tpr, 1/fpr, label='AUC = {:.2f}\\n $1/\\epsilon_{{bkg}}$(0.3) = {:.0f}'.format(auc_score, 1/fpr[closest_point(tpr, tpr_p=0.3)]))\n",
    "ax[1].plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[0].set_xlabel('$\\epsilon_{bkg}$ - FPR', fontproperties=axislabelfont)\n",
    "ax[0].set_ylabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "\n",
    "ax[1].set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax[1].set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(prop=axislabelfont)\n",
    "    ax[i].tick_params(labelsize=axisfontsize)\n",
    "    ax[i].grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "ax.hist(test_pred[:,0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs and translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation is <ins>**not**</ins> translation invariant but **equivariant**. what does that mean?\n",
    "Translation equivariant means that a translated input is mapped to the translated output, in formula:\n",
    "\n",
    "$f(x(t)) = y(t)$ &#10230; $f(x(t+s)) = y(t+s)$,\n",
    "\n",
    "while a translation invariant operation implies:\n",
    "\n",
    "$f(x(t)) = y(t)$ &#10230; $f(x(t+s)) = y(t)$.\n",
    "\n",
    "Nevertheless, we can check that the network is still invariant to small translation of the input datasets. Let's shift the test dataset in the vertical and horizontal axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h = torch.roll(z_test, 3, dims=3)\n",
    "test_v = torch.roll(z_test, 3, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the accuracy of the previously trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_size = 10000\n",
    "\n",
    "test_pred = model(z_test[:samp_size])\n",
    "test_pred_h = model(test_h[:samp_size])\n",
    "test_pred_v = model(test_v[:samp_size])\n",
    "\n",
    "test_pred_lab = torch.round(test_pred[:samp_size])\n",
    "test_pred_lab_h = torch.round(test_pred_h[:samp_size])\n",
    "test_pred_lab_v = torch.round(test_pred_v[:samp_size])\n",
    "\n",
    "test_cor = (test_pred_lab==y_test[:samp_size]).sum().item()\n",
    "test_cor_h = (test_pred_lab_h==y_test[:samp_size]).sum().item()\n",
    "test_cor_v = (test_pred_lab_v==y_test[:samp_size]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Accuracy on shifted datasets:\n",
      "No-shifts: 86.3 %\n",
      "Vertical shift: 74.6 %\n",
      "Horizontal shift: 82.1 %\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "separator()\n",
    "print('Accuracy on shifted datasets:')\n",
    "print('No-shifts: {:.1f} %'.format(test_cor*100/len(test_pred)))\n",
    "print('Vertical shift: {:.1f} %'.format(test_cor_h*100/len(test_pred_h)))\n",
    "print('Horizontal shift: {:.1f} %'.format(test_cor_v*100/len(test_pred_v)))\n",
    "separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Why there's some translation invariance? Why will it be broken at some point?\n",
    "\n",
    "max-pooling layer reduces the dependence on the spatial positioning. \n",
    "\n",
    "At large shifts we'll drastically change the physics of the jet (especially for tops), pooling doesn't help anymore, and linear layers are definitely not translation invariant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs vs FCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters does a CNN have?\n",
    "\n",
    "*output_channels* * (*input_channels* * *kernel_width* * *kernel_height* + 1)\n",
    "\n",
    "How many parameters does a FCN have?\n",
    "\n",
    "*output_nodes* * (input_nodes + 1)\n",
    "\n",
    "In our case we have 104 parameters from the first convolution, 100 from the second one, and 101 from the linear layer (no learnable weights in max-pooling and flattening), thus:\n",
    "\n",
    "**CNN parameters**: ~300\n",
    "\n",
    "Let's try to build a FCN with similar number of weights. The simplest network has just one layer that maps the pixels into a probability output.\n",
    "So, in the simplest case, we already have a number of parameters equal to the number of pixels.\n",
    "\n",
    "**FCN parameters** (simplest case): ~1600\n",
    "\n",
    "Let's try to train this FCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn parameters:  104, 100, 101 -> 300\n",
    "#smallest fcn has at least 1600 params\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_pixels=1600, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(img_pixels, out_dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            self.linear1,\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "FullyConnected(\n",
      "  (linear1): Linear(in_features=1600, out_features=2, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.695570  [    0/30000]\n",
      "current batch loss: 0.673624  [ 6400/30000]\n",
      "current batch loss: 0.631525  [12800/30000]\n",
      "current batch loss: 0.634810  [19200/30000]\n",
      "current batch loss: 0.577762  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.580474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.580014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.577143  [    0/30000]\n",
      "current batch loss: 0.586362  [ 6400/30000]\n",
      "current batch loss: 0.536305  [12800/30000]\n",
      "current batch loss: 0.523053  [19200/30000]\n",
      "current batch loss: 0.479259  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.508846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.507725\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.475137  [    0/30000]\n",
      "current batch loss: 0.498484  [ 6400/30000]\n",
      "current batch loss: 0.520698  [12800/30000]\n",
      "current batch loss: 0.468436  [19200/30000]\n",
      "current batch loss: 0.438889  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.461971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.460850\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.454992  [    0/30000]\n",
      "current batch loss: 0.508426  [ 6400/30000]\n",
      "current batch loss: 0.427921  [12800/30000]\n",
      "current batch loss: 0.502904  [19200/30000]\n",
      "current batch loss: 0.435495  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.430435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429105\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.439367  [    0/30000]\n",
      "current batch loss: 0.441485  [ 6400/30000]\n",
      "current batch loss: 0.379477  [12800/30000]\n",
      "current batch loss: 0.396214  [19200/30000]\n",
      "current batch loss: 0.488115  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.408594\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.407187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.424332  [    0/30000]\n",
      "current batch loss: 0.421743  [ 6400/30000]\n",
      "current batch loss: 0.401105  [12800/30000]\n",
      "current batch loss: 0.386007  [19200/30000]\n",
      "current batch loss: 0.370607  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.393055\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.391777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.440392  [    0/30000]\n",
      "current batch loss: 0.433327  [ 6400/30000]\n",
      "current batch loss: 0.356955  [12800/30000]\n",
      "current batch loss: 0.430123  [19200/30000]\n",
      "current batch loss: 0.287155  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.381793\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.380641\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.327231  [    0/30000]\n",
      "current batch loss: 0.396989  [ 6400/30000]\n",
      "current batch loss: 0.396541  [12800/30000]\n",
      "current batch loss: 0.363274  [19200/30000]\n",
      "current batch loss: 0.317056  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.373419\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.372345\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.359942  [    0/30000]\n",
      "current batch loss: 0.261191  [ 6400/30000]\n",
      "current batch loss: 0.281457  [12800/30000]\n",
      "current batch loss: 0.408467  [19200/30000]\n",
      "current batch loss: 0.344958  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.366984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.366234\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.401317  [    0/30000]\n",
      "current batch loss: 0.373006  [ 6400/30000]\n",
      "current batch loss: 0.335634  [12800/30000]\n",
      "current batch loss: 0.374721  [19200/30000]\n",
      "current batch loss: 0.253109  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.362106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.370940  [    0/30000]\n",
      "current batch loss: 0.342877  [ 6400/30000]\n",
      "current batch loss: 0.395056  [12800/30000]\n",
      "current batch loss: 0.310973  [19200/30000]\n",
      "current batch loss: 0.379446  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.358198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.358014\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.348625  [    0/30000]\n",
      "current batch loss: 0.359794  [ 6400/30000]\n",
      "current batch loss: 0.302752  [12800/30000]\n",
      "current batch loss: 0.355077  [19200/30000]\n",
      "current batch loss: 0.413559  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.355031\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.354846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.479218  [    0/30000]\n",
      "current batch loss: 0.266426  [ 6400/30000]\n",
      "current batch loss: 0.288561  [12800/30000]\n",
      "current batch loss: 0.413728  [19200/30000]\n",
      "current batch loss: 0.395846  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.352384\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.352328\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.367988  [    0/30000]\n",
      "current batch loss: 0.378130  [ 6400/30000]\n",
      "current batch loss: 0.348488  [12800/30000]\n",
      "current batch loss: 0.357140  [19200/30000]\n",
      "current batch loss: 0.352028  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.350180\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.350591\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.270567  [    0/30000]\n",
      "current batch loss: 0.366578  [ 6400/30000]\n",
      "current batch loss: 0.348553  [12800/30000]\n",
      "current batch loss: 0.292770  [19200/30000]\n",
      "current batch loss: 0.405192  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.348325\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.348813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.403012  [    0/30000]\n",
      "current batch loss: 0.338875  [ 6400/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.359884  [12800/30000]\n",
      "current batch loss: 0.240231  [19200/30000]\n",
      "current batch loss: 0.387258  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.346696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.347475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.446387  [    0/30000]\n",
      "current batch loss: 0.270833  [ 6400/30000]\n",
      "current batch loss: 0.193101  [12800/30000]\n",
      "current batch loss: 0.333969  [19200/30000]\n",
      "current batch loss: 0.364099  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.345264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.346232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.403643  [    0/30000]\n",
      "current batch loss: 0.503139  [ 6400/30000]\n",
      "current batch loss: 0.231527  [12800/30000]\n",
      "current batch loss: 0.408835  [19200/30000]\n",
      "current batch loss: 0.330589  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.343941\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.345212\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.350717  [    0/30000]\n",
      "current batch loss: 0.253666  [ 6400/30000]\n",
      "current batch loss: 0.204749  [12800/30000]\n",
      "current batch loss: 0.340064  [19200/30000]\n",
      "current batch loss: 0.266900  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.342786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.344269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.374784  [    0/30000]\n",
      "current batch loss: 0.351993  [ 6400/30000]\n",
      "current batch loss: 0.367487  [12800/30000]\n",
      "current batch loss: 0.312944  [19200/30000]\n",
      "current batch loss: 0.413003  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.341818\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.343615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.363181  [    0/30000]\n",
      "current batch loss: 0.230818  [ 6400/30000]\n",
      "current batch loss: 0.269949  [12800/30000]\n",
      "current batch loss: 0.355542  [19200/30000]\n",
      "current batch loss: 0.321614  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.340804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.311246  [    0/30000]\n",
      "current batch loss: 0.357987  [ 6400/30000]\n",
      "current batch loss: 0.245773  [12800/30000]\n",
      "current batch loss: 0.335481  [19200/30000]\n",
      "current batch loss: 0.290733  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339856\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.342018\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.409824  [    0/30000]\n",
      "current batch loss: 0.314263  [ 6400/30000]\n",
      "current batch loss: 0.398738  [12800/30000]\n",
      "current batch loss: 0.371710  [19200/30000]\n",
      "current batch loss: 0.337953  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.339029\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.285962  [    0/30000]\n",
      "current batch loss: 0.341105  [ 6400/30000]\n",
      "current batch loss: 0.405412  [12800/30000]\n",
      "current batch loss: 0.331320  [19200/30000]\n",
      "current batch loss: 0.307366  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.338235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.341050\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.350060  [    0/30000]\n",
      "current batch loss: 0.241133  [ 6400/30000]\n",
      "current batch loss: 0.400928  [12800/30000]\n",
      "current batch loss: 0.294277  [19200/30000]\n",
      "current batch loss: 0.311071  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.337473\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340465\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.374545  [    0/30000]\n",
      "current batch loss: 0.255300  [ 6400/30000]\n",
      "current batch loss: 0.311416  [12800/30000]\n",
      "current batch loss: 0.278733  [19200/30000]\n",
      "current batch loss: 0.291111  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336756\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.340166\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.378406  [    0/30000]\n",
      "current batch loss: 0.404049  [ 6400/30000]\n",
      "current batch loss: 0.280226  [12800/30000]\n",
      "current batch loss: 0.413177  [19200/30000]\n",
      "current batch loss: 0.348715  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336032\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339640\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.250442  [    0/30000]\n",
      "current batch loss: 0.233350  [ 6400/30000]\n",
      "current batch loss: 0.280817  [12800/30000]\n",
      "current batch loss: 0.329570  [19200/30000]\n",
      "current batch loss: 0.438524  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.335459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.339275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.389746  [    0/30000]\n",
      "current batch loss: 0.317585  [ 6400/30000]\n",
      "current batch loss: 0.315060  [12800/30000]\n",
      "current batch loss: 0.244339  [19200/30000]\n",
      "current batch loss: 0.423586  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334853\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338889\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.266142  [    0/30000]\n",
      "current batch loss: 0.422230  [ 6400/30000]\n",
      "current batch loss: 0.335501  [12800/30000]\n",
      "current batch loss: 0.381695  [19200/30000]\n",
      "current batch loss: 0.224469  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.334244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338490\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.316032  [    0/30000]\n",
      "current batch loss: 0.209536  [ 6400/30000]\n",
      "current batch loss: 0.377659  [12800/30000]\n",
      "current batch loss: 0.290617  [19200/30000]\n",
      "current batch loss: 0.362337  [25600/30000]\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg trn loss per batch: 0.333667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338280\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.387966  [    0/30000]\n",
      "current batch loss: 0.437692  [ 6400/30000]\n",
      "current batch loss: 0.253189  [12800/30000]\n",
      "current batch loss: 0.403831  [19200/30000]\n",
      "current batch loss: 0.409821  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.333125\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.338044\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.283063  [    0/30000]\n",
      "current batch loss: 0.229793  [ 6400/30000]\n",
      "current batch loss: 0.303127  [12800/30000]\n",
      "current batch loss: 0.382762  [19200/30000]\n",
      "current batch loss: 0.262915  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332668\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337611\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.285848  [    0/30000]\n",
      "current batch loss: 0.304517  [ 6400/30000]\n",
      "current batch loss: 0.414969  [12800/30000]\n",
      "current batch loss: 0.474801  [19200/30000]\n",
      "current batch loss: 0.376305  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.332214\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337560\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.363006  [    0/30000]\n",
      "current batch loss: 0.304513  [ 6400/30000]\n",
      "current batch loss: 0.364025  [12800/30000]\n",
      "current batch loss: 0.308497  [19200/30000]\n",
      "current batch loss: 0.341069  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331707\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.337078\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.286630  [    0/30000]\n",
      "current batch loss: 0.433164  [ 6400/30000]\n",
      "current batch loss: 0.293598  [12800/30000]\n",
      "current batch loss: 0.360600  [19200/30000]\n",
      "current batch loss: 0.457797  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.331236\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336933\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.285466  [    0/30000]\n",
      "current batch loss: 0.381854  [ 6400/30000]\n",
      "current batch loss: 0.323045  [12800/30000]\n",
      "current batch loss: 0.310508  [19200/30000]\n",
      "current batch loss: 0.391560  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336823\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.290659  [    0/30000]\n",
      "current batch loss: 0.317957  [ 6400/30000]\n",
      "current batch loss: 0.213783  [12800/30000]\n",
      "current batch loss: 0.240265  [19200/30000]\n",
      "current batch loss: 0.211994  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336238\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.341745  [    0/30000]\n",
      "current batch loss: 0.288213  [ 6400/30000]\n",
      "current batch loss: 0.414261  [12800/30000]\n",
      "current batch loss: 0.222041  [19200/30000]\n",
      "current batch loss: 0.319491  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329900\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336267\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.319443  [    0/30000]\n",
      "current batch loss: 0.322182  [ 6400/30000]\n",
      "current batch loss: 0.299425  [12800/30000]\n",
      "current batch loss: 0.384012  [19200/30000]\n",
      "current batch loss: 0.291969  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329583\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.336160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.392286  [    0/30000]\n",
      "current batch loss: 0.229925  [ 6400/30000]\n",
      "current batch loss: 0.362759  [12800/30000]\n",
      "current batch loss: 0.333419  [19200/30000]\n",
      "current batch loss: 0.273351  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.329194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.331350  [    0/30000]\n",
      "current batch loss: 0.341287  [ 6400/30000]\n",
      "current batch loss: 0.351544  [12800/30000]\n",
      "current batch loss: 0.336237  [19200/30000]\n",
      "current batch loss: 0.331771  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335917\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.321302  [    0/30000]\n",
      "current batch loss: 0.289248  [ 6400/30000]\n",
      "current batch loss: 0.301745  [12800/30000]\n",
      "current batch loss: 0.360537  [19200/30000]\n",
      "current batch loss: 0.252242  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335425\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.380239  [    0/30000]\n",
      "current batch loss: 0.265365  [ 6400/30000]\n",
      "current batch loss: 0.225507  [12800/30000]\n",
      "current batch loss: 0.296031  [19200/30000]\n",
      "current batch loss: 0.407211  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335291\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.288731  [    0/30000]\n",
      "current batch loss: 0.301025  [ 6400/30000]\n",
      "current batch loss: 0.374080  [12800/30000]\n",
      "current batch loss: 0.420967  [19200/30000]\n",
      "current batch loss: 0.288842  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327639\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335257\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.343376  [    0/30000]\n",
      "current batch loss: 0.320661  [ 6400/30000]\n",
      "current batch loss: 0.283411  [12800/30000]\n",
      "current batch loss: 0.370065  [19200/30000]\n",
      "current batch loss: 0.266515  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335288\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.296022  [    0/30000]\n",
      "current batch loss: 0.425522  [ 6400/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.288686  [12800/30000]\n",
      "current batch loss: 0.338700  [19200/30000]\n",
      "current batch loss: 0.269714  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.327043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335024\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.332966  [    0/30000]\n",
      "current batch loss: 0.338867  [ 6400/30000]\n",
      "current batch loss: 0.410062  [12800/30000]\n",
      "current batch loss: 0.375037  [19200/30000]\n",
      "current batch loss: 0.270178  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326688\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334990\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.289543  [    0/30000]\n",
      "current batch loss: 0.325476  [ 6400/30000]\n",
      "current batch loss: 0.366948  [12800/30000]\n",
      "current batch loss: 0.289486  [19200/30000]\n",
      "current batch loss: 0.289261  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326412\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334662\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.345608  [    0/30000]\n",
      "current batch loss: 0.292669  [ 6400/30000]\n",
      "current batch loss: 0.328589  [12800/30000]\n",
      "current batch loss: 0.524466  [19200/30000]\n",
      "current batch loss: 0.438748  [25600/30000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.326064\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.334416\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model = FullyConnected().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( train_cnn_dataloader, model, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( train_cnn_dataloader, model, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_cnn_dataloader, model, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "axs.plot( trn_losses, label=\"train loss\", color=c1 )\n",
    "axs.plot( val_losses, label=\"val   loss\", color=c2 )\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel( \"epoch\", fontproperties=axislabelfont )\n",
    "axs.set_ylabel( \"Binary CE\", fontproperties=axislabelfont )\n",
    "\n",
    "xticks = [ int(x) for x in axs.get_xticks() ]\n",
    "axs.set_xticklabels( xticks, fontproperties=tickfont )\n",
    "\n",
    "yticks = axs.get_yticks()\n",
    "axs.set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs.legend( loc='best', prop=tickfont )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg trn loss per batch: 0.326076\n",
      "avg val loss per batch: 0.339978\n",
      "avg val loss per batch: 0.334433\n",
      "-----------------------------------------------\n",
      "Final evaluation of performance:\n",
      "MSE-loss on train dataset: 0.3260761338002138\n",
      "MSE-loss on validation dataset: 0.33997837830581135\n",
      "MSE-loss on test dataset: 0.33443318275627554\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trn_loss = trn_pass( train_cnn_dataloader, model, loss_fn )\n",
    "val_loss = val_pass( test_cnn_dataloader, model, loss_fn )\n",
    "test_loss = val_pass( val_cnn_dataloader, model, loss_fn )\n",
    "\n",
    "separator()\n",
    "print(\"Final evaluation of performance:\")\n",
    "print(\"MSE-loss on train dataset: {}\".format(trn_loss))\n",
    "print(\"MSE-loss on validation dataset: {}\".format(val_loss))\n",
    "print(\"MSE-loss on test dataset: {}\".format(test_loss))\n",
    "separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_size = 10000\n",
    "\n",
    "train_pred = model(z_train[:samp_size]).detach()\n",
    "test_pred = model(z_test[:samp_size]).detach()\n",
    "val_pred = model(z_val[:samp_size]).detach()\n",
    "\n",
    "train_pred_lab = torch.round(train_pred[:samp_size])\n",
    "test_pred_lab = torch.round(test_pred[:samp_size])\n",
    "val_pred_lab = torch.round(val_pred[:samp_size])\n",
    "\n",
    "train_correct = (train_pred_lab==y_train[:samp_size]).sum().item()\n",
    "test_correct = (test_pred_lab==y_test[:samp_size]).sum().item()\n",
    "val_correct = (val_pred_lab==y_val[:samp_size]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Evaluation of accuracy: \n",
      "Accuracy on training dataset: 86.1 %\n",
      "Accuracy on validation dataset: 85.8 %\n",
      "Accuracy on test dataset: 85.5 %\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "separator()\n",
    "print('Evaluation of accuracy: ')\n",
    "print('Accuracy on training dataset: {:.1f} %'.format(train_correct*100/len(train_pred)))\n",
    "print('Accuracy on validation dataset: {:.1f} %'.format(val_correct*100/len(val_pred)))\n",
    "print('Accuracy on test dataset: {:.1f} %'.format(test_correct*100/len(test_pred)))\n",
    "separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-training?  Solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you mean over-fitting? \n",
    "\n",
    "Convolutions reduce over-fitting as we can see in the cnn vs fcn.  why? (weights sharing, invariance, less parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could we improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a larger model:\n",
    "- mora channels;\n",
    "- more convolutions;\n",
    "- smaller filters;\n",
    "\n",
    "with larger training dataset (at least 100000 imgs)\n",
    "\n",
    "... else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
